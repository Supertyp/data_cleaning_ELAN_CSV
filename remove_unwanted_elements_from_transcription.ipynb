{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcdac26",
   "metadata": {},
   "source": [
    "## Add columns to csv file with contitional entries\n",
    "read csv files from folder and add columns to each file with new entries based on the previous columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e786fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed CSV file: Bcnt_AEF_032_Camila.csv\n",
      "Processed CSV file: Bcnt_AEF_063_Daisy.csv\n",
      "Processed CSV file: Bcnt_AEF_065_Bernice.csv\n",
      "Processed CSV file: Bcnt_AEF_073_Marjorie.csv\n",
      "Processed CSV file: Bcnt_AEF_075_Agnes.csv\n",
      "Processed CSV file: Bcnt_AEF_079_Gladys.csv\n",
      "Processed CSV file: Bcnt_AEF_087_Grace.csv\n",
      "Processed CSV file: Bcnt_AEF_102_Evelyn.csv\n",
      "Processed CSV file: Bcnt_AEF_104_Mable.csv\n",
      "Processed CSV file: Bcnt_AEF_117_Audrey.csv\n",
      "Processed CSV file: Bcnt_AEF_118_Merl.csv\n",
      "Processed CSV file: Bcnt_AEF_132_Nellie.csv\n",
      "Processed CSV file: Bcnt_AEF_143_Dora.csv\n",
      "Processed CSV file: Bcnt_AEF_156_Edith.csv\n",
      "Processed CSV file: Bcnt_AEF_157_Jessica.csv\n",
      "Processed CSV file: Bcnt_AEF_158_Beatrice.csv\n",
      "Processed CSV file: Bcnt_AEF_176_Elsie.csv\n",
      "Processed CSV file: Bcnt_AEM_012_Fred.csv\n",
      "Processed CSV file: Bcnt_AEM_035_Jeffrey.csv\n",
      "Processed CSV file: Bcnt_AEM_072_Warren.csv\n",
      "Processed CSV file: Bcnt_AEM_078_Alfred.csv\n",
      "Processed CSV file: Bcnt_AEM_083_Theodore.csv\n",
      "Processed CSV file: Bcnt_AEM_106_Jacob.csv\n",
      "Processed CSV file: Bcnt_AEM_112_Woodrow.csv\n",
      "Processed CSV file: Bcnt_AEM_114_Bruce.csv\n",
      "Processed CSV file: Bcnt_AEM_116_Edmund.csv\n",
      "Processed CSV file: Bcnt_AEM_147_Roger.csv\n",
      "Processed CSV file: Bcnt_AEM_153_Frank.csv\n",
      "Processed CSV file: Bcnt_AEM_166_Harvey.csv\n",
      "Processed CSV file: Bcnt_AEM_178_Leonard.csv\n",
      "Processed CSV file: Bcnt_AEM_195_Charles.csv\n",
      "Processed CSV file: SSDS_AAF_024_Julie.csv\n",
      "Processed CSV file: SSDS_AAF_107_Ruth.csv\n",
      "Processed CSV file: SSDS_AAF_118_Jill.csv\n",
      "Processed CSV file: SSDS_AAF_125_Paula.csv\n",
      "Processed CSV file: SSDS_AAF_127_Janice.csv\n",
      "Processed CSV file: SSDS_AAF_151_Jade.csv\n",
      "Processed CSV file: SSDS_AAF_164_Belinda.csv\n",
      "Processed CSV file: SSDS_AAF_178_Brittany.csv\n",
      "Processed CSV file: SSDS_AAF_259_Carol.csv\n",
      "Processed CSV file: SSDS_AAF_303_Pam.csv\n",
      "Processed CSV file: SSDS_AAF_306_Wilma.csv\n",
      "Processed CSV file: SSDS_AAF_701_Jane.csv\n",
      "Processed CSV file: SSDS_AAM_021_Jim.csv\n",
      "Processed CSV file: SSDS_AAM_025_Lachlan.csv\n",
      "Processed CSV file: SSDS_AAM_163_Barry.csv\n",
      "Processed CSV file: SSDS_AAM_165_Keith.csv\n",
      "Processed CSV file: SSDS_AAM_305_Gregory.csv\n",
      "Processed CSV file: SSDS_AAM_501_Graham.csv\n",
      "Processed CSV file: SSDS_AAM_505_Hugh.csv\n",
      "Processed CSV file: SSDS_AAM_604_Bill.csv\n",
      "Processed CSV file: SSDS_ATF_006_Rhiannon.csv\n",
      "Processed CSV file: SSDS_ATF_026_Robin.csv\n",
      "Processed CSV file: SSDS_ATF_131_Fay.csv\n",
      "Processed CSV file: SSDS_ATF_159_Fiona.csv\n",
      "Processed CSV file: SSDS_ATF_166_Jeanie.csv\n",
      "Processed CSV file: SSDS_ATF_175_Evangeline.csv\n",
      "Processed CSV file: SSDS_ATF_181_Jo.csv\n",
      "Processed CSV file: SSDS_ATF_301_Kim.csv\n",
      "Processed CSV file: SSDS_ATF_304_Jenny.csv\n",
      "Processed CSV file: SSDS_ATF_503_Whitney.csv\n",
      "Processed CSV file: SSDS_ATF_601_Lily.csv\n",
      "Processed CSV file: SSDS_ATF_706_Tammy.csv\n",
      "Processed CSV file: SSDS_ATM_027_Steven.csv\n",
      "Processed CSV file: SSDS_ATM_028_Pete.csv\n",
      "Processed CSV file: SSDS_ATM_101_Brian.csv\n",
      "Processed CSV file: SSDS_ATM_102_Martin.csv\n",
      "Processed CSV file: SSDS_ATM_114_Philip.csv\n",
      "Processed CSV file: SSDS_ATM_126_Mike.csv\n",
      "Processed CSV file: SSDS_ATM_128_Mitchell.csv\n",
      "Processed CSV file: SSDS_ATM_167_Tim.csv\n",
      "Processed CSV file: SSDS_ATM_171_Richard.csv\n",
      "Processed CSV file: SSDS_ATM_180_Dylan.csv\n",
      "Processed CSV file: SSDS_ATM_702_Ryan.csv\n",
      "Processed CSV file: SSDS_ATM_703_Gary.csv\n",
      "Processed CSV file: SSDS_GTF_801_Elisa.csv\n",
      "Processed CSV file: SSDS_GTF_811_Agatha.csv\n",
      "Processed CSV file: SSDS_GTF_839_Pippa.csv\n",
      "Processed CSV file: SSDS_GTF_840_Kristina.csv\n",
      "Processed CSV file: SSDS_GTF_844_Linore.csv\n",
      "Processed CSV file: SSDS_GTF_852_SandraSSDS.csv\n",
      "Processed CSV file: SSDS_GTF_853_DinaSSDS.csv\n",
      "Processed CSV file: SSDS_GTF_855_Isa.csv\n",
      "Processed CSV file: SSDS_GTF_865_Hanna.csv\n",
      "Processed CSV file: SSDS_GTF_871_Samantha.csv\n",
      "Processed CSV file: SSDS_GTM_029_SimonSSDS.csv\n",
      "Processed CSV file: SSDS_GTM_030_George.csv\n",
      "Processed CSV file: SSDS_GTM_804_Ben.csv\n",
      "Processed CSV file: SSDS_GTM_822_Jack.csv\n",
      "Processed CSV file: SSDS_GTM_824_Andrew.csv\n",
      "Processed CSV file: SSDS_GTM_837_Damasus.csv\n",
      "Processed CSV file: SSDS_GTM_846_Carlo.csv\n",
      "Processed CSV file: SSDS_GTM_847_John.csv\n",
      "Processed CSV file: SSDS_GTM_848_Franco.csv\n",
      "Processed CSV file: SSDS_GTM_857_PatrickSSDS.csv\n",
      "Processed CSV file: SSDS_GTM_858_AmarSSDS.csv\n",
      "Processed CSV file: SSDS_GTM_859_Ren.csv\n",
      "Processed CSV file: SSDS_GTM_874_Anthony.csv\n",
      "Processed CSV file: SSDS_ITF_003_Marisa.csv\n",
      "Processed CSV file: SSDS_ITF_008_Maria.csv\n",
      "Processed CSV file: SSDS_ITF_009_Donna.csv\n",
      "Processed CSV file: SSDS_ITF_019_Blanca.csv\n",
      "Processed CSV file: SSDS_ITF_112_LilianaSSDS.csv\n",
      "Processed CSV file: SSDS_ITF_115_Lisa.csv\n",
      "Processed CSV file: SSDS_ITF_117_Eva.csv\n",
      "Processed CSV file: SSDS_ITF_120_Sara.csv\n",
      "Processed CSV file: SSDS_ITF_123_Marta.csv\n",
      "Processed CSV file: SSDS_ITF_124_Fabiana.csv\n",
      "Processed CSV file: SSDS_ITF_133_Violetta.csv\n",
      "Processed CSV file: SSDS_ITF_142_Celeste.csv\n",
      "Processed CSV file: SSDS_ITF_143_Lorraine.csv\n",
      "Processed CSV file: SSDS_ITM_012_DamienSSDS.csv\n",
      "Processed CSV file: SSDS_ITM_013_FabioSSDS.csv\n",
      "Processed CSV file: SSDS_ITM_017_AlbertoSSDS.csv\n",
      "Processed CSV file: SSDS_ITM_018_AlexiSSDS.csv\n",
      "Processed CSV file: SSDS_ITM_106_Dominic.csv\n",
      "Processed CSV file: SSDS_ITM_135_Adrian.csv\n",
      "Processed CSV file: SSDS_ITM_136_Alonso.csv\n",
      "Processed CSV file: SSDS_ITM_140_Luigi.csv\n",
      "Processed CSV file: SSDS_ITM_145_Murray.csv\n",
      "Processed CSV file: SSDS_ITM_147_Calvin.csv\n",
      "Processed CSV file: SSDS_ITM_149_David.csv\n",
      "Processed CSV file: SSDS_ITM_150_Eduardo.csv\n",
      "Processed CSV file: SydS_AOF_005_Jamie.csv\n",
      "Processed CSV file: SydS_AOF_006_Tessa.csv\n",
      "Processed CSV file: SydS_AOF_007_Annette.csv\n",
      "Processed CSV file: SydS_AOF_008_Shelley.csv\n",
      "Processed CSV file: SydS_AOF_009_Deidre.csv\n",
      "Processed CSV file: SydS_AOF_010_Paige.csv\n",
      "Processed CSV file: SydS_AOF_071_Naomi.csv\n",
      "Processed CSV file: SydS_AOF_080_Abigail.csv\n",
      "Processed CSV file: SydS_AOF_081_Jacqueline.csv\n",
      "Processed CSV file: SydS_AOF_082_Bailey.csv\n",
      "Processed CSV file: SydS_AOF_100_Katie.csv\n",
      "Processed CSV file: SydS_AOF_103_Darlene.csv\n",
      "Processed CSV file: SydS_AOF_118_Gillian.csv\n",
      "Processed CSV file: SydS_AOF_119_Danielle.csv\n",
      "Processed CSV file: SydS_AOF_126_Kimberley.csv\n",
      "Processed CSV file: SydS_AOM_064_Kenneth.csv\n",
      "Processed CSV file: SydS_AOM_067_Marshall.csv\n",
      "Processed CSV file: SydS_AOM_073_Perry.csv\n",
      "Processed CSV file: SydS_AOM_084_Tom.csv\n",
      "Processed CSV file: SydS_AOM_085_Glenn.csv\n",
      "Processed CSV file: SydS_AOM_086_Nigel.csv\n",
      "Processed CSV file: SydS_AOM_090_Geoff.csv\n",
      "Processed CSV file: SydS_AOM_097_Douglas.csv\n",
      "Processed CSV file: SydS_AOM_105_Parker.csv\n",
      "Processed CSV file: SydS_AOM_116_Dennis.csv\n",
      "Processed CSV file: SydS_AOM_120_Arthur.csv\n",
      "Processed CSV file: SydS_AOM_125_Paul.csv\n",
      "Processed CSV file: SydS_AYF_014_Hayley.csv\n",
      "Processed CSV file: SydS_AYF_016_Jocelyn.csv\n",
      "Processed CSV file: SydS_AYF_023_Nadia.csv\n",
      "Processed CSV file: SydS_AYF_028_Nina.csv\n",
      "Processed CSV file: SydS_AYF_033_Anna.csv\n",
      "Processed CSV file: SydS_AYF_035_Sophia.csv\n",
      "Processed CSV file: SydS_AYF_039_Annie.csv\n",
      "Processed CSV file: SydS_AYF_088_Emma.csv\n",
      "Processed CSV file: SydS_AYF_089_Sadie.csv\n",
      "Processed CSV file: SydS_AYF_092_Kylie.csv\n",
      "Processed CSV file: SydS_AYF_128_Amelia.csv\n",
      "Processed CSV file: SydS_AYF_130_Jasmine.csv\n",
      "Processed CSV file: SydS_AYM_012_Max.csv\n",
      "Processed CSV file: SydS_AYM_013_Tony.csv\n",
      "Processed CSV file: SydS_AYM_015_Zander.csv\n",
      "Processed CSV file: SydS_AYM_021_Matthew.csv\n",
      "Processed CSV file: SydS_AYM_050_Jaden.csv\n",
      "Processed CSV file: SydS_AYM_065_Adam.csv\n",
      "Processed CSV file: SydS_AYM_093_Trevor.csv\n",
      "Processed CSV file: SydS_AYM_101_Jeremy.csv\n",
      "Processed CSV file: SydS_AYM_104_Rhys.csv\n",
      "Processed CSV file: SydS_AYM_106_Larry.csv\n",
      "Processed CSV file: SydS_AYM_107_Reid.csv\n",
      "Processed CSV file: SydS_AYM_115_Nate.csv\n",
      "Processed CSV file: SydS_AYM_122_Nicholas.csv\n",
      "Processed CSV file: SydS_AYM_124_Craig.csv\n",
      "Processed CSV file: SydS_CYF_001_Sally.csv\n",
      "Processed CSV file: SydS_CYF_002_Karmen.csv\n",
      "Processed CSV file: SydS_CYF_003_Caroline.csv\n",
      "Processed CSV file: SydS_CYF_004_Charlotte.csv\n",
      "Processed CSV file: SydS_CYF_020_Alison.csv\n",
      "Processed CSV file: SydS_CYF_025_Amanda.csv\n",
      "Processed CSV file: SydS_CYF_031_Bridget.csv\n",
      "Processed CSV file: SydS_CYF_036_Rachel.csv\n",
      "Processed CSV file: SydS_CYF_047_Melissa.csv\n",
      "Processed CSV file: SydS_CYF_048_Sarah.csv\n",
      "Processed CSV file: SydS_CYF_069_Phoebe.csv\n",
      "Processed CSV file: SydS_CYM_019_Cameron.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed CSV file: SydS_CYM_030_Stuart.csv\n",
      "Processed CSV file: SydS_CYM_032_Isaac.csv\n",
      "Processed CSV file: SydS_CYM_038_Aaron.csv\n",
      "Processed CSV file: SydS_CYM_041_Gordon.csv\n",
      "Processed CSV file: SydS_CYM_051_Flynn.csv\n",
      "Processed CSV file: SydS_CYM_061_Mark.csv\n",
      "Processed CSV file: SydS_CYM_063_Callum.csv\n",
      "Processed CSV file: SydS_CYM_066_Kaleb.csv\n",
      "Processed CSV file: SydS_CYM_072_Lathan.csv\n",
      "Processed CSV file: SydS_CYM_075_Alec.csv\n",
      "Processed CSV file: SydS_GOF_139_Cassandra.csv\n",
      "Processed CSV file: SydS_GOF_142_Carmel.csv\n",
      "Processed CSV file: SydS_GOF_143_Laura.csv\n",
      "Processed CSV file: SydS_GOF_146_Cora.csv\n",
      "Processed CSV file: SydS_GOF_148_DinaSydS.csv\n",
      "Processed CSV file: SydS_GOF_153_SandraSydS.csv\n",
      "Processed CSV file: SydS_GOF_154_Lydia.csv\n",
      "Processed CSV file: SydS_GOF_155_Nikola.csv\n",
      "Processed CSV file: SydS_GOF_157_Meredith.csv\n",
      "Processed CSV file: SydS_GOF_158_Dorothy.csv\n",
      "Processed CSV file: SydS_GOF_159_Katrina.csv\n",
      "Processed CSV file: SydS_GOF_160_Alena.csv\n",
      "Processed CSV file: SydS_GOM_136_SimonSydS.csv\n",
      "Processed CSV file: SydS_GOM_138_Dario.csv\n",
      "Processed CSV file: SydS_GOM_144_AmarSydS.csv\n",
      "Processed CSV file: SydS_GOM_145_PatrickSydS.csv\n",
      "Processed CSV file: SydS_GYF_141_Athena.csv\n",
      "Processed CSV file: SydS_GYF_161_Cecilia.csv\n",
      "Processed CSV file: SydS_GYM_131_Toby.csv\n",
      "Processed CSV file: SydS_GYM_140_Nathaniel.csv\n",
      "Processed CSV file: SydS_GYM_149_Demitrius.csv\n",
      "Processed CSV file: SydS_GYM_156_Lewis.csv\n",
      "Processed CSV file: SydS_IOF_011_Diana.csv\n",
      "Processed CSV file: SydS_IOF_029_Deborah.csv\n",
      "Processed CSV file: SydS_IOF_042_Aida.csv\n",
      "Processed CSV file: SydS_IOF_053_Federica.csv\n",
      "Processed CSV file: SydS_IOF_055_Marianna.csv\n",
      "Processed CSV file: SydS_IOF_087_Maruccia.csv\n",
      "Processed CSV file: SydS_IOF_091_Renata.csv\n",
      "Processed CSV file: SydS_IOF_096_Carlotta.csv\n",
      "Processed CSV file: SydS_IOF_108_Daria.csv\n",
      "Processed CSV file: SydS_IOF_113_Giorgia.csv\n",
      "Processed CSV file: SydS_IOF_132_Simone.csv\n",
      "Processed CSV file: SydS_IOF_147_LilianaSydS.csv\n",
      "Processed CSV file: SydS_IOM_022_James.csv\n",
      "Processed CSV file: SydS_IOM_043_Mario.csv\n",
      "Processed CSV file: SydS_IOM_058_Vittorio.csv\n",
      "Processed CSV file: SydS_IOM_074_Giovanni.csv\n",
      "Processed CSV file: SydS_IOM_083_Enrico.csv\n",
      "Processed CSV file: SydS_IOM_095_Lorenzo.csv\n",
      "Processed CSV file: SydS_IOM_098_Paolo.csv\n",
      "Processed CSV file: SydS_IOM_109_Apollo.csv\n",
      "Processed CSV file: SydS_IOM_112_Diego.csv\n",
      "Processed CSV file: SydS_IOM_127_DamienSydS.csv\n",
      "Processed CSV file: SydS_IOM_134_AlbertoSydS.csv\n",
      "Processed CSV file: SydS_IOM_135_FabioSydS.csv\n",
      "Processed CSV file: SydS_IOM_152_AlexiSydS.csv\n",
      "Processed CSV file: SydS_IYF_037_Monica.csv\n",
      "Processed CSV file: SydS_IYF_040_Julia.csv\n",
      "Processed CSV file: SydS_IYF_054_Anita.csv\n",
      "Processed CSV file: SydS_IYF_056_Donatella.csv\n",
      "Processed CSV file: SydS_IYF_094_Gemma.csv\n",
      "Processed CSV file: SydS_IYF_111_Caterina.csv\n",
      "Processed CSV file: SydS_IYF_117_Elaine.csv\n",
      "Processed CSV file: SydS_IYF_133_Sienna.csv\n",
      "Processed CSV file: SydS_IYF_150_Gabriella.csv\n",
      "Processed CSV file: SydS_IYF_151_Roseta.csv\n",
      "Processed CSV file: SydS_IYM_068_Pietro.csv\n",
      "Processed CSV file: SydS_IYM_078_Pablo.csv\n",
      "Processed CSV file: SydS_IYM_079_Emilio.csv\n",
      "Processed CSV file: SydS_IYM_114_Samuel.csv\n",
      "Processed CSV file: SydS_IYM_129_Manuel.csv\n",
      "Processed CSV file: SydS_IYM_137_Stefano.csv\n",
      "+++ DONE +++\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "#folder_path = \"C:\\\\Users\\\\barth\\\\Documents\\\\LDACA\\\\data_cleaning\\\\transcripts_csv_test\"\n",
    "#folder_path = \"C:\\\\Users\\\\barth\\\\Documents\\\\LDACA\\\\data_cleaning\\\\transcripts_csv\"\n",
    "#folder_path = \"C:\\\\Users\\\\barth\\\\Documents\\\\LDACA\\\\data_cleaning\\\\test\"\n",
    "folder_path = \"C:\\\\Users\\\\barth\\\\Dropbox\\\\SydneySpeaks_for_ANU_library\\\\csv\"\n",
    "\n",
    "\n",
    "\n",
    "dataAll = {}\n",
    "\n",
    "key_dict = {'.': 'final', \n",
    "            ',': 'continuing', \n",
    "            '?': 'rising', \n",
    "            '--': 'truncated'\n",
    "            }\n",
    "\n",
    "def ends_with_any_key(input_string, key_dictionary):\n",
    "    \"\"\"does the puntuation at the end match the keys in the dictionary\"\"\"\n",
    "    for key in key_dictionary.keys():\n",
    "        if input_string.endswith(key):\n",
    "            return key_dictionary[key]\n",
    "    return 'NA'\n",
    "    \n",
    "    \n",
    "def wordCounter(wordList):\n",
    "    \"\"\"is it one word or more?\"\"\"\n",
    "    #cleanedWordList = remove_non_alphanumeric(wordList)\n",
    "            \n",
    "    if len(wordList) == 1:\n",
    "        return \"Y\"\n",
    "    elif len(wordList) == 0:\n",
    "        return '-'\n",
    "    return \"N\"\n",
    "\n",
    "def remove_spaces_before_punctuation(input_string):\n",
    "    return re.sub(r'\\s+([,.!?;:])', r'\\1', input_string)\n",
    "\n",
    "\n",
    "\n",
    "def filter_words(words_list):\n",
    "    filtered_list = [word for word in words_list if all(c.isalnum() or c in '~=!.,:;[]' for c in word)]\n",
    "    #print (filtered_list)\n",
    "    return filtered_list\n",
    "\n",
    "def remove_non_alphanumeric_strings(input_list):\n",
    "    cleaned_list = [item for item in input_list if any(c.isalnum() for c in item)]\n",
    "    return cleaned_list\n",
    "\n",
    "def split_string_with_punctuation(input_string):\n",
    "    pattern = r'\\s+|\\''\n",
    "    \"\"\"Use regex to split the string by words and punctuation\"\"\"\n",
    "    words = re.split(pattern, input_string)\n",
    "    #print (words)\n",
    "    words = filter_words(words)\n",
    "    words = remove_non_alphanumeric_strings(words)\n",
    "    return words\n",
    "\n",
    "def find_strings_with_symbols(string_list, symbol_list):\n",
    "    # find all strings from list which don't contain any of the symbols in the other list\n",
    "    filtered_strings = [s for s in string_list if not any(symbol in s for symbol in symbol_list)]\n",
    "    return filtered_strings\n",
    "\n",
    "def remove_all_x(s):\n",
    "    # Remove all occurrences of X (single or multiple)\n",
    "    return re.sub(r'X+', '', s)\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter out only CSV files\n",
    "csv_files = [file for file in file_list if file.endswith('.csv')]\n",
    "\n",
    "# Process CSV files\n",
    "for csv_file in csv_files:\n",
    "    dataAll[csv_file] = []\n",
    "    csv_file_path = os.path.join(folder_path, csv_file)\n",
    "    new_csv_file_path = os.path.join(folder_path, f\"clean\\{csv_file}\")\n",
    "    \n",
    "    dataAllFile = os.path.join(folder_path, f\"clean\\dataAll.csv\")\n",
    "\n",
    "    # Add headers and copy data to new CSV files\n",
    "    with open(csv_file_path, 'r', newline='', encoding=\"utf8\") as input_file, \\\n",
    "         open(new_csv_file_path, 'w', newline='',encoding=\"utf8\") as output_file:\n",
    "\n",
    "        csv_reader = csv.reader(input_file)\n",
    "        csv_writer = csv.writer(output_file)\n",
    "\n",
    "        # Read existing headers\n",
    "        existing_headers = next(csv_reader)\n",
    "\n",
    "        # Add new headers to existing headers\n",
    "        new_headers = existing_headers + [\"Intonation\", \"OneWordIU\",\"IU_clean\", \"text\"]\n",
    "        csv_writer.writerow(new_headers)\n",
    "\n",
    "        # Copy data from the old CSV to the new one\n",
    "        for row in csv_reader:\n",
    "            #print ('-------------------------')\n",
    "            #print (row)\n",
    "            \n",
    "            # add Intonation column\n",
    "            result = ends_with_any_key(row[5].strip(), key_dict)\n",
    "            modified_row = row + [result]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #add OneWordIU column\n",
    "            \n",
    "            # remove all comments between ( and )\n",
    "            wordString = row[5].strip()\n",
    "            \n",
    "            \n",
    "            \n",
    "            if (\"(\" in wordString) and (\")\" in wordString):\n",
    "                #print ('---------------------------')\n",
    "                #print (wordString)\n",
    "                res = re.findall(r'\\(.*?\\)', wordString)\n",
    "                #print (wordString)\n",
    "                for x in res:\n",
    "                    wordString = wordString.replace(x, \"\")\n",
    "                #print (wordString)\n",
    "                \n",
    "            \n",
    "            # remove all <This  and that>\n",
    "            if \"<\" in wordString:\n",
    "                #print ('---------------------------')\n",
    "                #print (wordString)\n",
    "                \n",
    "                res = re.findall(r\"<\\S*\", wordString)\n",
    "                \n",
    "                for x in res:\n",
    "                    wordString = wordString.replace(x, \"\")\n",
    "                #print (wordString)\n",
    "            \n",
    "            if \">\" in wordString:\n",
    "                #print ('---------------------------')\n",
    "                #print (wordString)\n",
    "                res = re.findall(r\"\\S*>\", wordString)\n",
    "                \n",
    "                for x in res:\n",
    "                    wordString = wordString.replace(x, \"\")\n",
    "                #print (wordString)\n",
    "            \n",
    "            \n",
    "            #print (wordString)\n",
    "            wordList = split_string_with_punctuation(wordString)\n",
    "            #print (wordList)\n",
    "            result2 = wordCounter(wordList)\n",
    "            #print (result2)\n",
    "            \n",
    "            # override result for special case of word with symbols inside lile '[y]es'\n",
    "            if (len(row[5].strip()) > 0 ) and (' ' not in row[5].strip()):\n",
    "                result2 = 'Y'\n",
    "                \n",
    "            modified_row2 = modified_row + [result2]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # clean the IU column and add a column with the clean text\n",
    "            \n",
    "            rawString = row[5].strip()\n",
    "            \n",
    "            rawString = rawString.replace('\\t', '')\n",
    "            rawString = rawString.replace('\\n', '')\n",
    "            rawString = rawString.replace('’', \"'\")\n",
    "            \n",
    "            # for devbugging\n",
    "            if 'Xfrrr' in rawString:\n",
    "                print ('---------------------------')\n",
    "                printer = True\n",
    "            else:\n",
    "                printer = False\n",
    "                \n",
    "            if printer:\n",
    "                print(rawString)\n",
    "            \n",
    "            pattern = r\"\\(\\((.*?)\\)\\)\"\n",
    "\n",
    "            matches = re.findall(pattern, rawString)\n",
    "            matchesComplete = [f\"(({match}))\" for match in matches] \n",
    "            for each in matchesComplete:\n",
    "                rawString = rawString.replace(each, '')\n",
    "\n",
    "            \n",
    "            \n",
    "            #remove '[Number' and 'Number]'\n",
    "            patterns = [r\"\\[\\d\", r\"\\d\\]\"]\n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, rawString)\n",
    "                for match in matches:\n",
    "                    rawString = rawString.replace(match, '')\n",
    "\n",
    "            \n",
    "            rawList = rawString.split()\n",
    "            \n",
    "            \n",
    "            newList = []\n",
    "            for each in rawList:\n",
    "                pattern = r\"!\\w+[^A-Za-z0-9_]\"\n",
    "\n",
    "                matches = re.findall(pattern, each)\n",
    "                if len(matches) > 0:\n",
    "                    newList.append(matches[0][1:])\n",
    "                else:\n",
    "                    newList.append(each)\n",
    "                \n",
    "                    \n",
    "            rawList = newList   \n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "            # keep punctuation after > \n",
    "            newList = []\n",
    "            for each in rawList:\n",
    "                if '>' in each:\n",
    "                    pattern = r\">([^\\w\\s])\"\n",
    "                    matches = re.findall(pattern, each)\n",
    "                    if (len(matches) > 0) and (len(newList) > 0):\n",
    "                        newList[-1] = newList[-1] + matches[0]\n",
    "                else:\n",
    "                    newList.append(each)\n",
    "            #print (newList)\n",
    "            rawList = newList\n",
    "                    \n",
    "            \n",
    "            \n",
    "            # filter out all words from list which have one of the sybmols in it\n",
    "            # symbols to be filtered out:\n",
    "            symbols = [\n",
    "                       \"...\",\n",
    "                       \"..\",\n",
    "                       \"…(N.N)\",\n",
    "                       \"!\",\n",
    "                       \"X \",\n",
    "                       \"(\",\n",
    "                       \")\",\n",
    "                       \"<\",\n",
    "                       \">\",\n",
    "                       \"%\"]\n",
    "            \n",
    "            \n",
    "            filtered_strings = find_strings_with_symbols(rawList, symbols)\n",
    "            \n",
    "            outString = \" \".join(filtered_strings)\n",
    "            \n",
    "            outString = remove_all_x(outString)\n",
    "            \n",
    "            # replacement of some characters\n",
    "            replaceDict = {\"[\": \"\", \n",
    "                           \"]\": \"\",\n",
    "                           \"=\": \"\",\n",
    "                           \"~\": \"\",\n",
    "                           \"@\": \"\",\n",
    "                           \"--\": \"\", \n",
    "                           \"- \": \" \",\n",
    "                           \" -\": \" \"}\n",
    "            \n",
    "            for k, v in replaceDict.items():\n",
    "                outString = outString.replace(k, v)\n",
    "               \n",
    "\n",
    "            if printer:\n",
    "                print(outString)\n",
    "                \n",
    "            result3 = outString\n",
    "            \n",
    "            modified_row3 = modified_row2 + [result3]\n",
    "            \n",
    "            \n",
    "            # version without the Xs\n",
    "            result4 = outString.replace('X', '')\n",
    "            result4 = remove_spaces_before_punctuation(result4)\n",
    "            \n",
    "            dataAll[csv_file].append(result4)\n",
    "            \n",
    "            \n",
    "            if printer:\n",
    "                print (result4)\n",
    "            modified_row4 = modified_row3 + [result4]\n",
    "            \n",
    "            csv_writer.writerow(modified_row4)\n",
    "\n",
    "    print(f\"Processed CSV file: {csv_file}\")\n",
    "    \n",
    "print (\"+++ DONE +++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84795b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Done +++\n"
     ]
    }
   ],
   "source": [
    "# create a single file for all texts from the corpus\n",
    "def filter_utf8_strings(strings):\n",
    "    utf8_strings = []\n",
    "    for string in strings:\n",
    "        try:\n",
    "            string.encode('utf-8').decode('utf-8')\n",
    "            utf8_strings.append(string)\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    return utf8_strings\n",
    "\n",
    "def remove_multiple_spaces(input_string):\n",
    "    return re.sub(r'\\s+', ' ', input_string)\n",
    "\n",
    "def write_dict_to_csv(dictionary, file_name):\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['text_name', 'text'])  # Write header\n",
    "\n",
    "        for key, value in dictionary.items():\n",
    "            \n",
    "            value = filter_utf8_strings(value)\n",
    "            \n",
    "            cleanString = \"\"\n",
    "            for each in value:\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                if len(each) > 0: \n",
    "                    cleanString = cleanString + each.strip() + \" \"\n",
    "                    \n",
    "            \n",
    "            cleanString = remove_multiple_spaces(cleanString)\n",
    "                \n",
    "               \n",
    "                \n",
    "            \n",
    "            csv_writer.writerow([key, cleanString])\n",
    "    #print (cleanString)\n",
    "# Example dictionary\n",
    "\n",
    "# Example usage\n",
    "write_dict_to_csv(dataAll, dataAllFile)\n",
    "\n",
    "print (\"+++ Done +++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb243f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ DONE +++\n"
     ]
    }
   ],
   "source": [
    "def write_dict_to_files(dictionary):\n",
    "    for key, value in dictionary.items():\n",
    "        newName = key[:-3] + \"txt\"\n",
    "        new_txt_file_path = os.path.join(folder_path, f\"clean\\\\transcripts\\{newName}\")\n",
    "        with open(new_txt_file_path, 'w', encoding=\"utf-8\") as file:\n",
    "            value = filter_utf8_strings(value)\n",
    "            \n",
    "            cleanString = \"\"\n",
    "            for each in value:\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                if len(each) > 0: \n",
    "                    cleanString = cleanString + each.strip() + \" \"\n",
    "                    \n",
    "            \n",
    "            cleanString = remove_multiple_spaces(cleanString)\n",
    "            \n",
    "            file.write(cleanString)\n",
    "\n",
    "write_dict_to_files(dataAll)\n",
    "print (\"+++ DONE +++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c1eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
